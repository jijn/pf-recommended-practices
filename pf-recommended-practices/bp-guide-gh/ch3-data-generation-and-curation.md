# Data Generation and Curation

- *[Trevor Keller](https://www.nist.gov/people/trevor-keller), NIST*, [@tkphd]
- *[Daniel Wheeler](https://www.nist.gov/people/daniel-wheeler), NIST*, [@wd15]
- *[Damien Pinto](https://ca.linkedin.com/in/damien-pinto-4748387b), McGill*, [@DamienPinto]

# Data Generation and Curation

## Overview


Phase field models are characterized by a form of PDE related to an
Eulerian free boundary problem and defined by a diffuse
interface. Phase field models for practical applications require
sufficient high fidelity to resolve both the macro length scale
related to the application and the micro length scales associated with
the free boundary. Performing a useful phase field simulation requires
extensive computationally resources and can generate large volumes of
raw field data. This data consists of field variables defined
frequently across a domain or an interpolation function with many
Gauss points. Typically, data is stored at sufficient temporal
frequency to reconstruct the evolution of the field variables.

In recent years there have been efforts to embed phase field models
into integrated computataional materials engineering (ICME) based
materials design workflows {cite}`TOURRET2022100810`. However, to
leverage relevant phase field resources for these workflows a
systematic approach is required for archiving and accessing
data. Furthermore, it is often difficult for downstream researchers to
find and access raw or minimally processed data from phase field
studies, before the post-processing steps and final publication. In
this document, we will provide motivation, guidance and a template for
packaging and publishing findable, accessible, interoperable and
reusable (FAIR) data from phase field studies as well as managing
unpublished raw data {cite}`Wilkinson2016`. Following the protocols
outlined in this guide will provide downstream researchers with an
enhanced capability to use phase field as part of larger ICME
workflows and, in particular, data intensive usages such as AI
surrogate models. This guide serves as a primer rather than a
detailied reference on scientific data, aiming to stimulate thought
and ensure that phase field practitioners are aware of the key
considerations before initiating a phase field study.

## Definitions

It is beneficial for the reader to be clear regarding the main
concepts of FAIR data management when applied to phase field
studies. Broadly speaking, **FAIR data management** encompasses the
curation of simulation workflows (including the software, data inputs
and data outputs) for subsequent researchers or even machine
agents. **FAIR data** concepts for simulation workflows have been well
explained elsewhere, see
{cite}`wilkinson2024applyingfairprinciplescomputational`. A
**scientific workflow** is generally conceptualized as a graph of
connected actions with various inputs and outputs. Some of the nodes
in a workflow may not be entirely automated and require human agent
inputs, which can increase the complexity of workflow
curation. Workflow nodes include the pre and post-processing steps for
phase field simulation workflows. In this guide, the **raw and
post-processed data** is considered to be distinct from the
**metadata**, which describes the simulation, associated workflow and
data files. The **raw data** is generated by the simulation as it is
running and often consists of temporal field data. The
**post-processed data** consists of derived quantities and images
generated using the **raw data**. The **software** or **software
application** used to perform the simulation generally refers to a
particular phase field code, which is part of the larger
**computational environment**. The **code** might also refer to the
**software**, but the distinction is that the **code** may have been
modified by the researcher and might include **input files** to the
**software application**. Although the code can be considered as data
in the larger sense, in this work, the data curation process excludes
consideration of code curation, which involves its own distinct
practices. See the [Software Development](label-software-development)
section of the best practices guide for a more detailed discussion of
software and code curation.

```{mermaid}
---
title: A Phase Field Workflow
---p
flowchart TD
    id1@{ shape: lean-r, label: "Input Files", fill: #f96 }
    id1.2@{ shape: lean-r, label: "Parameters" }
    id1.1([Code])
    id2([Computational Environment])
    id2.5@{ shape: rect, label: "Pre-processing, (e.g. CALPHAD or Meshing)" }
    id2.7@{ shape: bow-rect, label: "Pre-processed Data" }
	id3[Phase Field Simulation]
	id3.5@{ shape: bow-rect, label: "Scratch Data" }
	id4@{ shape: bow-rect, label: "Raw Field Data" }
	id5@{ shape: rect, label: "Post-processing (e.g. Data Visualization)" }
	id6@{ shape: bow-rect, label: "Post-processed Data" }
	id7@{ shape: lin-cyl, label: "Data Repository" }
	id1.2-->id1
	id1-->id2.5
	id1-->id5
	id2.5-->id2.7-->id3
	id1-->id3
	id1.1-->id3
	id2-->id3
	id3-->id4-->id5-->id6
	id3-->id3.5-->id3
	id2-->id2.5
	id2-->id5
	id6--Curation-->id7
	id4--Curation-->id7
```

## Data Generation

Let's first draw the distinction between **data generation** and **data
curation**. Data generation involves writing raw data to disk during the
simulation execution and generating post-processed data from that raw
data. Data curation involves packaging the generated data objects from
a phase field workflow or study along with sufficient provenance
metadata into a FAIR research object for consumption by subsequent
scientific studies.

When performing a phase field simulation, one must be cognizant of
several factors pertaining to data generation. Generally speaking, the
considerations can be defined as follow,

- choosing data to generate (and then curate),
- file formats,
- file system hierarchy,
- restarts and recovering from crashes
- data generation  and workflow tools, and
- HPC environments and writing to disk in parallel.

These considerations are often conflicting, require trial and error to
determine the best approach and are highly specific to the
requirements of the workflow and post-processing. However, there are
some general guidelines that will be outlined below.

### Choosing data to generate

Selecting the appropriate data to write to disk during the simulation
largely depends on the requirements such as post-processing or
debugging. However, it is good practice to consider future uses of the
data for future work such as subsequent researchers trying to reuse
the workflow or even reviewers. Lack of forethought in saving data
could hinder the data curation of the eventual curation of the data
research object. This step should be considered independently from
restarts. The data required to reconstruct derived quantities or the
evolution of field data will not be the same as the data required to
restart a simulation.

Another aspect of saving data to disk is the frequency off the disk
writes. This choice can often impact the performance of a simulation
as the simulation might have to wait on the disk before continuing the
computation. A method to avoid this is to use a separate thread that
runs concurrently to write the data (in the same process), see
[Stackflow
Question](https://stackoverflow.com/questions/1014113/efficient-way-to-save-data-to-disk-while-running-a-computationally-intensive-tas). In
fact many practitioners overlook optimizing this part of aspect of
phase field codes
[ref](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202410). Generally,
when writing data it is best to do single large write to disk as
opposed to multiple small writes. In practice this could involve
caching multiple field variables across multiple print steps as a
single data blob to an HDF5 file. However, there is a trade off
between simulation performance and memory usage as well as latency and
communication overhead when considering parallel simulations.

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0202410
https://dl.acm.org/doi/abs/10.1145/1713072.1713079

### File formats

In general, when running phase field simulations, the user is limited
to the file format that the software supports. For example, if the
research is using PRISMS-PF the default data format is VTK and there
is no reason to seek an alternative. If an alternative file format is
required then the researcher could code a C++ function to write data
in an alternative format to VTK such as NetCDF.

As a general rule it is best to choose file formats that work with the
tools already in use and / or that your colleagues are using. There
are other considerations to be aware of though. Human readable formats
such as CSV and JSON are often useful for small medium data sets (such
as derived quantities) as some metadata can be embedded alongside the
raw data resulting in a FAIRer data product than standard binary
formats. Some binary file formats also support metadata and might be
more useful for final data curation of a phase field study even if not
used during the research process. One main benefit of using binary
data (beyond saving disk space) is the ability to preserve full
precision for floating point numbers. The longevity of file formats
should be considered as well. A particularly egregious case of
ignoring longevity would be using the Pickle file format in Python,
which is both language dependent and code dependent. It is an example
of data serialization, which is used mainly for in-process data
storage for asynchronous tasks, but not good for long term data
storage.

There are many binary formats used for storing field data based on an
Eulerian mesh or grid. Common formats for field data are NetCDF, VTK,
XDMF and EXODUS. Within the phase field community, VTK seems to be the
mostly widely used. VTK is actually a visualization library, but
supports a number of different native file formats based on both XML
and HDF5 (both non-binary and binary). The VTK library works well with
FE simulations supporting many different element types as well as
parallel data storage for domain decomposition. See the [XML file
formats
documentation](https://docs.vtk.org/en/latest/design_documents/VTKFileFormats.html#xml-file-formats)
for VTK for an overview of zoo of different file extensions and their
meaning. In contrast to VTK, NetCDF is more geared towards gridded
data having arisen from atmospheric research, which uses more FD and
FV than FE. For a comparison of performance and metrics for different
file types see the [Python MeshIO tools's
README.md](https://github.com/nschloe/meshio?tab=readme-ov-file#performance-comparison)

The Python MeshIO tool is a good place to start for IO when writing
custom phase field codes in Python (or Julia using `pyimport`). MeshIO
is also a good place to start for exploring, debugging or picking
apart file data in an interactive Python environment, which can be
harder to do with dedicated viewing tools like Paraview. The
scientific Python ecosystem is very rich with tools for data
manipulation and storage such as Pandas, which supports storage in
many different formats, and xarray for higher dimensional data. xarray
supports NetCDF file storage, which includes coordinate systems and
metadata in HDF5. Both Pandas and xarray can be used in a parallel or
a distributed manner in conjucntion with Dask. Dask along with xarray
supports writing to the Zarr data format. Zarr allows data to be
stored on disk during analysis to avoid loading the entire data object
into memory.

https://aaltoscicomp.github.io/python-for-scicomp/work-with-data/
https://docs.vtk.org/en/latest/index.html
https://docs.xarray.dev/en/stable/user-guide/io.html=

### Recovering from crashes and restarts

A study from 2020 of HPC systems calculated the success rate (I.e. no
error code on completion) of multi-node jobs with non-shared memory at
between 60 and 70 %
[Kumar](https://engineering.purdue.edu/dcsl/publications/papers/2020/fresco_dsn20_cameraready.pdf). This
success rate diminishes rapidly as the run time of jobs
increases. Needless to say that check-pointing is absolutely required
for any jobs of more than a few hours. Nearly everyday, an HPC
platform will experience some sort of failure
[Benoit1](https://inria.hal.science/hal-03264047/file/rr9413.pdf)
[Aupy](https://www.sciencedirect.com/science/article/abs/pii/S0743731513002219). That
doesn't mean that every job will fail everyday, but it would be
optimistic to think that jobs will go beyond a week without some
issues. Given that fact one can estimate how long it might take to run
a job without check-pointing. A very rough estimate for expected
completion time assuming instantaneous restarts and no queuing time is
given by,

$$ E(T) = \frac{1}{2} \left(1 + e^{\frac{T}{\mu}} \right) T $$

where $T$ is the nominal job completion time with no failures and
$\mu$ is the mean time to failure. The formula predicts an expected
time of 3.8 days for a job that nominally runs for 3 days with a $\mu$
of one week. The formula is of course a gross simplification and
includes many invalid assumptions, but regardless of the assumed
failure distribution the exponential time increase without
check-pointing is inescapable. Assuming that we're agreed on the need
for checkpoint, the next step is to decide on the optimal time
interval between checkpoints. This is given by the well known
Young/Daly formula, $W=\sqrt{2 \mu C}$, where $C$ is the time taken
for a checkpoint
[Benoit2](https://icl.utk.edu/files/publications/2022/icl-utk-1569-2022.pdf)
[Bautista-Gomez](https://www.ittc.ku.edu/~sun/publications/fgcs24.pdf). The
Young/Daly formula accounts for the trade off between the start up
time cost for a job to get back to its original point of failure and
the cost associated with writing the checkpoint to disk.  For example,
with a weekly failure rate and $C=6$ minutes, $W=5.8$ hours. In
practice these estimates for $\mu$ and $C$ might be a little
pessimistic, but be aware of the trade off. [Benoit1] . Note that some
HPC systems have upper bounds on run times (e.g. TACC has a 7 days
time limit so $\mu<7$ days regardless of other system failures).

Given the above theory, what is the some practical advice for
check-pointing jobs?

- Estimate both $\mu$ and $C$. It might be worth discussing the $\mu$
  value with the HPC cluster administrator to get some valid
  numbers. Of course $C$ can be estimated by running test jobs. It's
  good to know if you should be writing checkpoints every day or every
  hour or every minute.
- Ensure that restarts are deterministic (i.e. results don't change
  between a job that restarts and one that doesn't). One way to do
  this is to hash output files assuming that the simulation itself is
  deterministic
- Consider using a checkpointing library if you're using a custom
  phase field code or even a workflow tool such as Snakemake which has
  the inbuilt ability to handle checkpointing. A tool like Snakemake
  is good for large parameter studies where it is difficult to keep
  track of which jobs wrote which files. The `pickle` library is
  acceptable for checkpointint Python programs in this shortlived
  circumstance.
- Use the inbuilt check-pointing available in the phase field code
  that you're using.
- Whatever system is being used check that the check-pointing actually
  works and is deterministic.

- https://hivehpc.haifa.ac.il/index.php/slurm?start=5
- https://icl.utk.edu/files/publications/2022/icl-utk-1569-2022.pdf
- https://inria.hal.science/hal-03264047/file/rr9413.pdf
- https://www.sciencedirect.com/science/article/abs/pii/S0743731513002219
- https://icl.utk.edu/files/publications/2022/icl-utk-1569-2022.pdf
- https://www.ittc.ku.edu/~sun/publications/fgcs24.pdf
- https://www.ittc.ku.edu/~sun/publications/fgcs24.pdf
- https://icl.utk.edu/files/publications/2020/icl-utk-1385-2020.pdf
- https://ftp.cs.toronto.edu/csrg-technical-reports/621/ut-csrg-621.pdf
- https://arxiv.org/pdf/2012.00825
- https://icl.utk.edu/~herault/papers/007%20-%20Checkpointing%20Strategies%20for%20Shared%20High-Performance%20Computing%20Platforms%20-%20IJNC%20(2019).pdf
- https://dl.acm.org/doi/10.1145/2184512.2184574
- https://engineering.purdue.edu/dcsl/publications/papers/2020/fresco_dsn20_cameraready.pdf
- [Job failures](https://pdf.sciencedirectassets.com/271503/1-s2.0-S0898122111X00251/1-s2.0-S0898122111005980/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCnYz7Yg2JHorkw2CwX7PI5fbyLRr02ykVPbgtxZhNy8QIhAIY%2BTq58bdBe3iRdnRXNP%2FjQ0%2B4LgrXUQh7aakHn9TSTKrsFCID%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQBRoMMDU5MDAzNTQ2ODY1IgwlVGRSpcXtIR5IRTcqjwUcZrP%2B7%2Byn4dYmelmvfF1CCdKNMP%2BftdY1KdvKA%2BnlBpDHwh%2FulDxZPkotPpiaFnrHfT85QaPGB0Q5Ck16mfWIG5KAjrrPFXY2azR3%2FxLIM6I8Ka4aHzcvUDa5L2rn8PpHqVF61wtBWRZYI8N0YM5CZi8r2%2B6NLe9OJvgLR1%2B55%2BfwK5GucDahcWDrP2FvbBFWQyidEBNl7thbpO4NIKoUTGJkb8H%2BBezk09N%2F4CPCjHel5mA1CHA8cQLH9lcCPiLurzKTBP8ozNi%2FtrlAZUKRKk%2BYHMy5HyFl%2Bobumh1eesuGe19b%2FpYOZBGzrQ4mn9eblczLd2SQi3k%2FoAws7yW9HHqzmMkJnla2B3tgfpP9WxaCnb5ZMNLIjlwEfu67ZiydyWQ2VnygTjG8CsiBYUeFuCbTplAeviP4WN%2BtiK%2FAsXbQZW93Q7cH7K%2B7lhfPiuPaauh0tlgQJNVdp2QzT8qvsxbQtzdEOQ5ethcoNbXU8YmXYgYdUtGwQbySD18i3aQo6zdUD0h9YtNNl%2BXskT8nv9xVPzsfMdkvrWA23PdIDYagX6n4Dd45DkeYIa10oXQfQKy7JiYIyfy7L1zt6tE6Rr0H9aMogJZIfjZG3tFcRea97xN%2BputMKCCqpyz%2FBJgfCptLvjhoKsWCfIqiE23xTHTTl%2FTkTK20ZQ8yO8lHuKHwtjbJJrX%2BQgnfiZQp2Sm6sKWchwej1Nn8IgtbHswKexMqoyaQHUeokSJ6MwQmtHAoUjuwuhaVcOqHn5gNEBuAWo3pAS%2Fwn2TTG5g00gF7RpQRoZvflp4b9poAN20kS%2F0lmLzutQ3wHOq1Ak59OzQhZASygvTGiqCxRp%2BzqctUt6%2B3%2FKRFwhR7q%2BwRluPBMMyix7sGOrAB3n1WiLFCV6WV5KLzctnyXliLNDqpIxUPVeXy2v%2FvcR8zUDFo49QquQ6nJudq2u9aUJ4nKzEkzpLTdAOCnQ%2FIR68LdMPQF%2BqJr0BD78PqcfoaacB%2BH4vV0FhCOVs0rVLdevPVGoAhB%2BIFbrsv%2BvHvQUZxU3wX%2FuOp2ChL1cUohEeoQzo2PyF2FZXkNcUenB2EWhcZGpgAIL1EYIyu1iCYmbfjVlISQL7xMJHj%2BSq1H3c%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241230T003505Z&X-Amz-SignedHeaders=host&X-Amz-Expires=299&X-Amz-Credential=ASIAQ3PHCVTYXOJ65CE4%2F20241230%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=4d3b88d3c0b7d80e5ff0df11ef0d4246875d4aabdc0c8d49be54c9c029beae8e&hash=ebead6fd98beaced790a9b84e76f9f6326f41ebbf4cedadb1fde7cfc734bfc31&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0898122111005980&tid=spdf-4974a446-6433-4619-afca-a5c210488d71&sid=a6179c5a4b870940603b745364079c847353gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=161d5b095903025d02&rr=8f9df2a7da4fc990&cc=us)
- https://www.cs.cmu.edu/~bianca/dsn06.pdf
<!-- the note -->

### Using Workflow Tools

The authors of this article use Snakemake for their workflows so will
discuss this in particular, but most of the ideas will apply to other
workflow tools. In general when running many phase field jobs for a
parameter study or dealing with many pre and post-processing steps, it
is wise to employ a workflow tool such as Snakemake. One of the main
benefits of workflow tools is the automation of all the steps in a
workflow that researchers often neglect to implement in the absence of
a workflow tool (e.g. with bash scripts). This forces a structure and
the researchers to think carefully about the inputs / outputs and task
graph. As a side effect, the graph structure produces a much FAIRer
research object when the research is published and shared and even so
that the researcher can rerun the simulation steps in the future. For
example, when using Snakemake, the `Snakefile` itself is a clear
record of the steps required to re-execute the workflow. Ideally, the
`Snakefile` will include all the steps required to go from the raw
inputs to images and data tables used in publications, but this might
not always be possible.

A secondary impact of using a workflow tool is that it often imposes a
directory and file structure on the project. For example, Snakemake
has an ideal suggested structure. An example folder structure when
using Snakemake would look like the following.

```plain
.
├── config
│   └── config.yaml
├── LICENSE.md
├── README.md
├── resources
├── results
│   └── image.png
└── workflow
    ├── envs
    │   ├── env.yaml
    │   ├── flake.lock
    │   ├── flake.nix
    │   ├── poetry.lock
    │   └── pyproject.toml
    ├── notebooks
    │   └── analysis.ipynb
    ├── rules
    │   ├── postprocess.smk
    │   ├── preprocess.smk
    │   └── sim.smk
    ├── scripts
    │   ├── func.py
    │   └── run.py
    └── Snakefile
```

Notice that the above directory strucuture includes the `envs`
directory. This allows diffferent steps in the workflow to be run in
diffferent types of environments. The benefit of this is that the
steps can be highly hetrogeneous in terms of the required
computational enviornment. Additionally, most workflow tools will
support both HPC and local workstation execution and make porting
between systems easier.

See https://pmc.ncbi.nlm.nih.gov/articles/PMC8114187/ for a more
details overview off Snakemake and a list of other good workflow
tools.

- https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html


## Data Curation

Data curation involves the steps required to turn an unstructured data
from a research project into a coherent research data object
satisfying the principles of FAIR data. A robust data curation process
is often a requirement for compliance for funding requirements and to
simply meet the most basic needs of transparency in scientific
research. The main benefits of data curation include (see
[DCC](https://www.dcc.ac.uk/guidance/how-guides/develop-data-plan#Why%20develop))

Simulation FAIR data paragraph and importance of metadata

The fundamental steps to curate a computational research project into
a research data object and publish are as follows.

- Automate the entire computational workflow where possible during the
  research process from initial inputs to final research products such
  as images and data tables.
- Publish the code and workflows appropriately during development (see
  the ... guide).
- Employ a suitable metadata standard where possible to describe
  different aspects of the research project such as the raw data
  files, derived data assets, software environments, numerical
  algorithms and problems specification.
- Identify the significant raw and derived data assets that are
  required to produce the final research products.
- License the research work appropriately. This may require a separate
  license for the data products as they are generally not archived in
  the code repository.
- Select a data repository to curate the data
- Obtain a DOI for the data object and link with other research
  products

The above steps are difficult to implement near the conclusion of a
research project. The authors suggest implementing the majority of
these steps at the outset of the project and developing these steps as
part of a protocol for all research projects within a computational
materials research group.

### Automation

Automating workflows in computational materials science is useful for
many reasons, however, for data curation purposed it provides and
added benefit. In short, an outlined workflow associated with a
curated FAIR object is a major way to improve FAIR quality for
subsequent researchers. For most workflow tools, the operation script
outlining the workflow graph is the ultimate form of metadata about
how the archived data files are used or generated during the
research. For example, with Snakemake, the `Snakefile` has clearly
outlined inputs and outputs as well as the procedure associated with
each input / output pair. In particular, the computational
environment, command line arguments, environment variables are
recorded as well as the order of execution for each step. 

In recent years there have been efforts in the life sciences to
provide a minimum workflow for independent code execution during the
peer review process. The [CODECHECK
initiative](https://doi.org/10.12688/f1000research.51738.2) trys to
provide a standard for executing workflows and a certification if the
workflow satisifies basic criteria. These types of efforts will likely
be used within the compuational materials science community in the
coming years so adopting automated workflow tools as part of your
research will greatly benefit this process.

- https://www.sciencedirect.com/science/article/pii/S2666389921001707?via%3Dihub

### Metadata Standards

### Publish the codes and workflows during development

### Identifying the significant data assets

### Licensing

### Selecting a data repository

Dockstore and Workflowhub https://arxiv.org/pdf/2410.03490

## References

```{bibliography}
:filter: docname in docnames
```

<!-- links -->

[@tkphd]: https://github.com/tkphd
[@wd15]: https://github.com/wd15
[@DamienPinto]: https://github.com/DamienPinto
[CodeMeta]: https://codemeta.github.io
[CodeMeta Generator]: https://codemeta.github.io/codemeta-generator/
[FAIR Principles]: https://www.go-fair.org/fair-principles/
[PFHub]: https://pages.nist.gov/pfhub
[PFHub repository on GitHub]: https://github.com/usnistgov/pfhub
[Schema.org]: https://www.schema.org
[Zenodo]: https://zenodo.org
[fair-phase-field]: https://doi.org/10.5281/zenodo.7254581
[schemaorg]: https://github.com/openschemas/schemaorg
[structured data schema]: https://en.wikipedia.org/wiki/Data_model
[link1]: https://workflows.community/groups/fair/best-practices/

